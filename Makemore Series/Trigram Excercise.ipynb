{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\deepgcn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda:0\"\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of characters (a -> z)\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "chars = [\".\"] + chars\n",
    "\n",
    "# # make a dictionary of character to index\n",
    "stoi = {ch: i for (i, ch) in enumerate(chars)}\n",
    "\n",
    "# # make a dictionary of index to character\n",
    "itos = {i: ch for (ch, i) in stoi.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.ones(27, 27, 27, dtype = torch.int32, device = device)\n",
    "N[0, 0, 0] = 0\n",
    "# getting the Bigrams\n",
    "for w in words:\n",
    "    # add start and end tokens\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "\n",
    "        N[ix1, ix2, ix3] += 1\n",
    "\n",
    "P = N / N.sum(dim = 2, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quealiellyrol\n",
      "luwaqsh\n",
      "joecherazr\n",
      "ummann\n",
      "fa\n",
      "donit\n",
      "br\n",
      "zaelgle\n",
      "sh\n",
      "umic\n",
      "mosinslkk\n",
      "in\n",
      "bexto\n",
      "hast\n",
      "velyne\n",
      "zuri\n",
      "dar\n",
      "phitzssakathamorgia\n",
      "den\n",
      "quil\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator(device=device).manual_seed(1122)\n",
    "for i in range(20):\n",
    "  out = []\n",
    "  index1 = 0\n",
    "  index2 = 0\n",
    "  while True:\n",
    "    p = P[index1,index2]\n",
    "    out.append(itos[index1])\n",
    "    index1 = index2\n",
    "    index2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    if index2 == 0 :\n",
    "      break\n",
    "  print(''.join(out).replace(\".\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training set\n",
    "xs , ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    # add start and end tokens\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "\n",
    "        xs.append([ix1, ix2])\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs, dtype=torch.int64).to(device)\n",
    "ys = torch.tensor(ys, dtype=torch.int64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dataset = len(xs)\n",
    "train_num =  int(num_dataset * 0.8)\n",
    "\n",
    "train_x = xs[:train_num]\n",
    "train_y = ys[:train_num]\n",
    "\n",
    "val_x = xs[train_num:]\n",
    "val_y = ys[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: 3.7958  Val: 3.8308\n",
      " Train: 3.2885  Val: 3.4444\n",
      " Train: 3.0320  Val: 3.2426\n",
      " Train: 2.8811  Val: 3.1234\n",
      " Train: 2.7778  Val: 3.0390\n",
      " Train: 2.7011  Val: 2.9742\n",
      " Train: 2.6416  Val: 2.9225\n",
      " Train: 2.5938  Val: 2.8800\n",
      " Train: 2.5544  Val: 2.8443\n",
      " Train: 2.5214  Val: 2.8139\n",
      " Train: 2.4933  Val: 2.7876\n",
      " Train: 2.4690  Val: 2.7646\n",
      " Train: 2.4479  Val: 2.7443\n",
      " Train: 2.4294  Val: 2.7264\n",
      " Train: 2.4129  Val: 2.7103\n",
      " Train: 2.3983  Val: 2.6957\n",
      " Train: 2.3851  Val: 2.6826\n",
      " Train: 2.3731  Val: 2.6706\n",
      " Train: 2.3623  Val: 2.6596\n",
      " Train: 2.3524  Val: 2.6495\n",
      " Train: 2.3433  Val: 2.6401\n",
      " Train: 2.3349  Val: 2.6315\n",
      " Train: 2.3271  Val: 2.6234\n",
      " Train: 2.3199  Val: 2.6159\n",
      " Train: 2.3131  Val: 2.6088\n",
      " Train: 2.3068  Val: 2.6022\n",
      " Train: 2.3010  Val: 2.5960\n",
      " Train: 2.2954  Val: 2.5901\n",
      " Train: 2.2902  Val: 2.5846\n",
      " Train: 2.2853  Val: 2.5794\n",
      " Train: 2.2807  Val: 2.5744\n",
      " Train: 2.2763  Val: 2.5697\n",
      " Train: 2.2722  Val: 2.5652\n",
      " Train: 2.2682  Val: 2.5610\n",
      " Train: 2.2645  Val: 2.5569\n",
      " Train: 2.2609  Val: 2.5531\n",
      " Train: 2.2575  Val: 2.5494\n",
      " Train: 2.2543  Val: 2.5459\n",
      " Train: 2.2512  Val: 2.5425\n",
      " Train: 2.2482  Val: 2.5393\n",
      " Train: 2.2454  Val: 2.5362\n",
      " Train: 2.2427  Val: 2.5332\n",
      " Train: 2.2401  Val: 2.5304\n",
      " Train: 2.2376  Val: 2.5276\n",
      " Train: 2.2352  Val: 2.5250\n",
      " Train: 2.2329  Val: 2.5225\n",
      " Train: 2.2307  Val: 2.5201\n",
      " Train: 2.2286  Val: 2.5177\n",
      " Train: 2.2265  Val: 2.5154\n",
      " Train: 2.2245  Val: 2.5133\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1122)\n",
    "\n",
    "W = torch.randn((27,27,27), requires_grad = True, device = device)\n",
    "def calculate_loss(x,y):\n",
    "    num =  len(x)\n",
    "    xenc = F.one_hot(x, num_classes = 27).float().to(device)\n",
    "    first_char_enc = xenc[:,0,:] @ W.view(-1,W.shape[1] * W.shape[2])\n",
    "    logits = xenc[:,1,:].unsqueeze(1) @ first_char_enc.view(len(xenc),W.shape[1],W.shape[2])\n",
    "    result_exp = logits.exp()\n",
    "    prob = result_exp / result_exp.sum(2, keepdims=True)\n",
    "    loss = -prob.view(result_exp.shape[0],result_exp.shape[2])[torch.arange(num), y].log().mean() \n",
    "    # loss += 0.1*(W**2).mean()\n",
    "    return loss\n",
    "\n",
    "for k in range(500):\n",
    "    train_loss = calculate_loss(train_x,train_y)\n",
    "    W.grad = None\n",
    "    train_loss.backward()\n",
    "    with torch.no_grad():\n",
    "        val_loss = calculate_loss(val_x,val_y)\n",
    "        if k % 10 == 0:\n",
    "            print(f\" Train: {train_loss.item():.4f}  Val: {val_loss.item():.4f}\")\n",
    "        W -= 40 * W.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E04: we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: 3.6952  \n",
      " Train: 3.1927  \n",
      " Train: 2.9385  \n",
      " Train: 2.7890  \n",
      " Train: 2.6866  \n",
      " Train: 2.6106  \n",
      " Train: 2.5515  \n",
      " Train: 2.5039  \n",
      " Train: 2.4648  \n",
      " Train: 2.4318  \n",
      " Train: 2.4037  \n",
      " Train: 2.3795  \n",
      " Train: 2.3583  \n",
      " Train: 2.3397  \n",
      " Train: 2.3232  \n",
      " Train: 2.3085  \n",
      " Train: 2.2952  \n",
      " Train: 2.2832  \n",
      " Train: 2.2723  \n",
      " Train: 2.2623  \n",
      " Train: 2.2531  \n",
      " Train: 2.2446  \n",
      " Train: 2.2367  \n",
      " Train: 2.2294  \n",
      " Train: 2.2226  \n",
      " Train: 2.2162  \n",
      " Train: 2.2103  \n",
      " Train: 2.2047  \n",
      " Train: 2.1994  \n",
      " Train: 2.1944  \n",
      " Train: 2.1897  \n",
      " Train: 2.1852  \n",
      " Train: 2.1810  \n",
      " Train: 2.1770  \n",
      " Train: 2.1731  \n",
      " Train: 2.1695  \n",
      " Train: 2.1660  \n",
      " Train: 2.1627  \n",
      " Train: 2.1595  \n",
      " Train: 2.1565  \n",
      " Train: 2.1536  \n",
      " Train: 2.1508  \n",
      " Train: 2.1482  \n",
      " Train: 2.1456  \n",
      " Train: 2.1431  \n",
      " Train: 2.1408  \n",
      " Train: 2.1385  \n",
      " Train: 2.1363  \n",
      " Train: 2.1342  \n",
      " Train: 2.1322  \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1122)\n",
    "W = torch.randn((27,27,27), requires_grad = True, device = device)\n",
    "num = len(train_x)\n",
    "\n",
    "for k in range(500):\n",
    "    logits = W[train_x[:,0],train_x[:,1]]\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), train_y].log().mean()\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W -= 40 * W.grad\n",
    "        if k % 10 == 0 :\n",
    "            print(f\" Train: {loss.item():.4f}  \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E05: look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train: 3.6952  \n",
      " Train: 3.1927  \n",
      " Train: 2.9385  \n",
      " Train: 2.7890  \n",
      " Train: 2.6866  \n",
      " Train: 2.6106  \n",
      " Train: 2.5515  \n",
      " Train: 2.5039  \n",
      " Train: 2.4648  \n",
      " Train: 2.4318  \n",
      " Train: 2.4037  \n",
      " Train: 2.3795  \n",
      " Train: 2.3583  \n",
      " Train: 2.3397  \n",
      " Train: 2.3232  \n",
      " Train: 2.3085  \n",
      " Train: 2.2952  \n",
      " Train: 2.2832  \n",
      " Train: 2.2723  \n",
      " Train: 2.2623  \n",
      " Train: 2.2531  \n",
      " Train: 2.2446  \n",
      " Train: 2.2367  \n",
      " Train: 2.2294  \n",
      " Train: 2.2226  \n",
      " Train: 2.2162  \n",
      " Train: 2.2103  \n",
      " Train: 2.2047  \n",
      " Train: 2.1994  \n",
      " Train: 2.1944  \n",
      " Train: 2.1897  \n",
      " Train: 2.1852  \n",
      " Train: 2.1810  \n",
      " Train: 2.1770  \n",
      " Train: 2.1731  \n",
      " Train: 2.1695  \n",
      " Train: 2.1660  \n",
      " Train: 2.1627  \n",
      " Train: 2.1596  \n",
      " Train: 2.1565  \n",
      " Train: 2.1536  \n",
      " Train: 2.1508  \n",
      " Train: 2.1482  \n",
      " Train: 2.1456  \n",
      " Train: 2.1432  \n",
      " Train: 2.1408  \n",
      " Train: 2.1385  \n",
      " Train: 2.1363  \n",
      " Train: 2.1342  \n",
      " Train: 2.1322  \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1122)\n",
    "W = torch.randn((27,27,27), requires_grad = True, device = device)\n",
    "num = len(train_x)\n",
    "for k in range(500):\n",
    "    logits = W[train_x[:,0],train_x[:,1]]\n",
    "    loss = F.cross_entropy(logits, train_y)\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W -= 40 * W.grad\n",
    "        if k % 10 == 0 :\n",
    "            print(f\" Train: {loss.item():.4f}  \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E06: meta-exercise! Think of a fun/interesting exercise and complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
