{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nn.LSTM output tensor([[[0.0991, 0.1728]]], grad_fn=<MkldnnRnnLayerBackward0>) \n",
            "Manual output tensor([[0.0991, 0.1728]], grad_fn=<MulBackward0>) \n",
            " \n",
            " \n",
            "nn.LSTM hidden tensor([[[0.0991, 0.1728]]], grad_fn=<StackBackward0>)\n",
            "Manual hidden tensor([[0.0991, 0.1728]], grad_fn=<MulBackward0>)\n",
            " \n",
            " \n",
            "nn.LSTM state tensor([[[0.1023, 0.9999]]], grad_fn=<StackBackward0>)\n",
            "manual state tensor([[0.1023, 0.9999]], grad_fn=<AddBackward0>)\n",
            "tensor([[0.0918, 1.0000]], grad_fn=<SigmoidBackward0>) tensor([[1.0000, 0.9998]], grad_fn=<TanhBackward0>) tensor([[0.2151, 0.0219]], grad_fn=<SigmoidBackward0>) tensor([[0.9447, 0.1202]], grad_fn=<SigmoidBackward0>)\n",
            "official:tensor([[[0.0991, 0.1728]]], grad_fn=<MkldnnRnnLayerBackward0>)\n",
            "step:tensor([[0.0865, 0.0915]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "batchsize = 1\n",
        "\n",
        "# nn.LSTM \n",
        "torch.manual_seed(20)\n",
        "lstm_official = torch.nn.LSTM(6, 2, bidirectional=False, num_layers=1, batch_first=False)\n",
        "share_weight = torch.randn(lstm_official.weight_ih_l0.shape,dtype = torch.float)\n",
        "lstm_official.weight_ih_l0 = torch.nn.Parameter(share_weight)\n",
        "# bias set to zeros\n",
        "lstm_official.bias_ih_l0 = torch.nn.Parameter(torch.rand(lstm_official.bias_ih_l0.shape))\n",
        "lstm_official.weight_hh_l0 = torch.nn.Parameter(torch.rand(lstm_official.weight_hh_l0.shape))\n",
        "# bias set to zeros\n",
        "lstm_official .bias_hh_l0 = torch.nn.Parameter(torch.zeros(lstm_official.bias_ih_l0.shape))\n",
        "x = torch.tensor([[1,2,3,4,5,6],[1,2,3,4,5,6]],dtype=torch.float)\n",
        "lstm_official_out = lstm_official(x[0].unsqueeze(dim=0).unsqueeze(dim=0))\n",
        "\n",
        "# manual implementation\n",
        "W_ii, W_if, W_ig, W_io = lstm_official.weight_ih_l0.split(2, dim=0)\n",
        "b_ii, b_if, b_ig, b_io = lstm_official.bias_ih_l0.split(2, dim=0)\n",
        "\n",
        "W_hi, W_hf, W_hg, W_ho = lstm_official.weight_hh_l0.split(2, dim=0)\n",
        "b_hi, b_hf, b_hg, b_ho = lstm_official.bias_hh_l0.split(2, dim=0)\n",
        "\n",
        "input = x[0].unsqueeze(0)\n",
        "prev_h = torch.zeros((batchsize,2))\n",
        "prev_c = torch.zeros((batchsize,2))\n",
        "\n",
        "i_t = torch.sigmoid(F.linear(input, W_ii, b_ii) + F.linear(prev_h, W_hi, b_hi))\n",
        "f_t = torch.sigmoid(F.linear(input, W_if, b_if) + F.linear(prev_h, W_hf, b_hf))\n",
        "g_t = torch.tanh(F.linear(input, W_ig, b_ig) + F.linear(prev_h, W_hg, b_hg))\n",
        "o_t = torch.sigmoid(F.linear(input, W_io, b_io) + F.linear(prev_h, W_ho, b_ho))\n",
        "c_t = f_t * prev_c + i_t * g_t\n",
        "h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "print('nn.LSTM output {} '.format(lstm_official_out[0]))\n",
        "print('Manual output {} '.format(h_t))\n",
        "print(' ')\n",
        "print(' ')\n",
        "print('nn.LSTM hidden {}'.format(lstm_official_out[1][0]))\n",
        "print('Manual hidden {}'.format(h_t))\n",
        "print(' ')\n",
        "print(' ')\n",
        "print('nn.LSTM state {}'.format(lstm_official_out[1][1]))\n",
        "print('manual state {}'.format( c_t))\n",
        "\n",
        "# implementation step by step\n",
        "out_shape=2\n",
        "batchsize=1\n",
        "i2h = nn.Linear(in_features=6, out_features=8)  #\n",
        "h2h = nn.Linear(in_features=out_shape, out_features=8)\n",
        "i2h.weight = torch.nn.Parameter(share_weight)\n",
        "i2h.bias = torch.nn.Parameter(torch.zeros(i2h.bias.shape))\n",
        "h2h.weight = torch.nn.Parameter(torch.ones(h2h.weight.shape))\n",
        "h2h.bias = torch.nn.Parameter(torch.zeros(h2h.bias.shape))\n",
        "\n",
        "x_i2h = i2h(x[0].unsqueeze(dim=0)) \n",
        "prev_h = torch.zeros((batchsize,2))\n",
        "prev_c = torch.zeros((batchsize,2))\n",
        "x_h2h = h2h(prev_h)\n",
        "gates = x_i2h + x_h2h\n",
        "gates = torch.split(gates,out_shape,-1)\n",
        "in_gate = torch.sigmoid(gates[0])  \n",
        "in_transform = torch.tanh(gates[2])\n",
        "forget_gate = torch.sigmoid(gates[1]) \n",
        "out_gate = torch.sigmoid(gates[3]) \n",
        "print(in_gate,in_transform,forget_gate,out_gate)\n",
        "s0 = forget_gate * prev_c\n",
        "s1 = in_gate * in_transform\n",
        "next_c = s0 + s1\n",
        "next_h = out_gate * F.tanh(next_c)  \n",
        "\n",
        "print(f'official:{lstm_official_out[0]}')\n",
        "print(f'step:{next_h}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 6])\n",
            "torch.Size([8, 2])\n",
            "torch.Size([8])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "for parameter in lstm_official.parameters():\n",
        "    print(parameter.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 15\n",
        "feature_dims = 15\n",
        "hidden_dim = 10\n",
        "n_layers = 2\n",
        "\n",
        "torch.manual_seed(0)\n",
        "x_input = torch.rand(batch_size,feature_dims)\n",
        "lstm = nn.LSTM(feature_dims, hidden_dim, n_layers, batch_first=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0269,  0.1329,  0.0217,  0.0395, -0.0449, -0.0714, -0.0431,  0.0161,\n",
              "          0.0531, -0.0313],\n",
              "        [ 0.0402,  0.2106,  0.0347,  0.0601, -0.0631, -0.0974, -0.0695,  0.0404,\n",
              "          0.0992, -0.0276],\n",
              "        [ 0.0581,  0.2592,  0.0377,  0.0799, -0.0733, -0.1129, -0.0862,  0.0681,\n",
              "          0.1214, -0.0388],\n",
              "        [ 0.0500,  0.2893,  0.0407,  0.0784, -0.0674, -0.1103, -0.0979,  0.0946,\n",
              "          0.1432, -0.0320],\n",
              "        [ 0.0683,  0.3032,  0.0354,  0.0944, -0.0646, -0.1272, -0.0895,  0.1162,\n",
              "          0.1557, -0.0376],\n",
              "        [ 0.0737,  0.3076,  0.0495,  0.1004, -0.0631, -0.1258, -0.0936,  0.1065,\n",
              "          0.1661, -0.0323],\n",
              "        [ 0.0762,  0.3140,  0.0624,  0.0981, -0.0539, -0.1150, -0.0944,  0.0926,\n",
              "          0.1671, -0.0251],\n",
              "        [ 0.0856,  0.3165,  0.0598,  0.1075, -0.0547, -0.1226, -0.1020,  0.0958,\n",
              "          0.1702, -0.0269],\n",
              "        [ 0.0978,  0.3176,  0.0633,  0.1161, -0.0569, -0.1311, -0.1109,  0.0895,\n",
              "          0.1708, -0.0337],\n",
              "        [ 0.0787,  0.3279,  0.0628,  0.1011, -0.0486, -0.1204, -0.1217,  0.0967,\n",
              "          0.1652, -0.0338],\n",
              "        [ 0.0977,  0.3277,  0.0692,  0.1020, -0.0472, -0.1108, -0.1181,  0.0885,\n",
              "          0.1665, -0.0411],\n",
              "        [ 0.0872,  0.3298,  0.0545,  0.0984, -0.0514, -0.1166, -0.1217,  0.0990,\n",
              "          0.1591, -0.0392],\n",
              "        [ 0.0838,  0.3205,  0.0522,  0.0986, -0.0494, -0.1200, -0.1062,  0.0990,\n",
              "          0.1657, -0.0306],\n",
              "        [ 0.0888,  0.3237,  0.0583,  0.1072, -0.0444, -0.1282, -0.1083,  0.0959,\n",
              "          0.1611, -0.0454],\n",
              "        [ 0.0890,  0.3146,  0.0568,  0.1100, -0.0383, -0.1400, -0.1030,  0.0926,\n",
              "          0.1579, -0.0437]], grad_fn=<SqueezeBackward1>)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out, hidden = lstm(x_input)\n",
        "out"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
