{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  2.,  3.],\n",
              "          [ 4.,  5.,  6.,  7.],\n",
              "          [ 8.,  9., 10., 11.],\n",
              "          [12., 13., 14., 15.]],\n",
              "\n",
              "         [[16., 17., 18., 19.],\n",
              "          [20., 21., 22., 23.],\n",
              "          [24., 25., 26., 27.],\n",
              "          [28., 29., 30., 31.]],\n",
              "\n",
              "         [[32., 33., 34., 35.],\n",
              "          [36., 37., 38., 39.],\n",
              "          [40., 41., 42., 43.],\n",
              "          [44., 45., 46., 47.]]],\n",
              "\n",
              "\n",
              "        [[[48., 49., 50., 51.],\n",
              "          [52., 53., 54., 55.],\n",
              "          [56., 57., 58., 59.],\n",
              "          [60., 61., 62., 63.]],\n",
              "\n",
              "         [[64., 65., 66., 67.],\n",
              "          [68., 69., 70., 71.],\n",
              "          [72., 73., 74., 75.],\n",
              "          [76., 77., 78., 79.]],\n",
              "\n",
              "         [[80., 81., 82., 83.],\n",
              "          [84., 85., 86., 87.],\n",
              "          [88., 89., 90., 91.],\n",
              "          [92., 93., 94., 95.]]]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B, C, H, W = 2, 3, 4 , 4\n",
        "\n",
        "x = torch.arange(0,B*C*H*W).view(B,C,H,W).float()\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[-1.2889, -1.2480, -1.2071, -1.1662],\n",
              "          [-1.1253, -1.0843, -1.0434, -1.0025],\n",
              "          [-0.9616, -0.9207, -0.8798, -0.8388],\n",
              "          [-0.7979, -0.7570, -0.7161, -0.6752]],\n",
              "\n",
              "         [[-1.2889, -1.2480, -1.2071, -1.1662],\n",
              "          [-1.1253, -1.0843, -1.0434, -1.0025],\n",
              "          [-0.9616, -0.9207, -0.8798, -0.8388],\n",
              "          [-0.7979, -0.7570, -0.7161, -0.6752]],\n",
              "\n",
              "         [[-1.2889, -1.2480, -1.2071, -1.1662],\n",
              "          [-1.1253, -1.0843, -1.0434, -1.0025],\n",
              "          [-0.9616, -0.9207, -0.8798, -0.8388],\n",
              "          [-0.7979, -0.7570, -0.7161, -0.6752]]],\n",
              "\n",
              "\n",
              "        [[[ 0.6752,  0.7161,  0.7570,  0.7979],\n",
              "          [ 0.8388,  0.8798,  0.9207,  0.9616],\n",
              "          [ 1.0025,  1.0434,  1.0843,  1.1253],\n",
              "          [ 1.1662,  1.2071,  1.2480,  1.2889]],\n",
              "\n",
              "         [[ 0.6752,  0.7161,  0.7570,  0.7979],\n",
              "          [ 0.8388,  0.8798,  0.9207,  0.9616],\n",
              "          [ 1.0025,  1.0434,  1.0843,  1.1253],\n",
              "          [ 1.1662,  1.2071,  1.2480,  1.2889]],\n",
              "\n",
              "         [[ 0.6752,  0.7161,  0.7570,  0.7979],\n",
              "          [ 0.8388,  0.8798,  0.9207,  0.9616],\n",
              "          [ 1.0025,  1.0434,  1.0843,  1.1253],\n",
              "          [ 1.1662,  1.2071,  1.2480,  1.2889]]]],\n",
              "       grad_fn=<NativeBatchNormBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batchnorm = nn.BatchNorm2d(C)\n",
        "batchnorm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([3.1500, 4.7500, 6.3500]), tensor([62.5516, 62.5516, 62.5516]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batchnorm.running_mean, batchnorm.running_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([1., 1., 1.], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0., 0., 0.], requires_grad=True))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batchnorm.weight,batchnorm.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1e-05"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batchnorm.eps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[22.6667, 22.6667, 22.6667],\n",
              "        [22.6667, 22.6667, 22.6667]])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.view(B, C, H* W ).var(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 7.5000, 23.5000, 39.5000],\n",
              "        [55.5000, 71.5000, 87.5000]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.view(B, C, H* W ).mean(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[256., 256., 256., 256.],\n",
              "         [256., 256., 256., 256.],\n",
              "         [256., 256., 256., 256.],\n",
              "         [256., 256., 256., 256.]],\n",
              "\n",
              "        [[256., 256., 256., 256.],\n",
              "         [256., 256., 256., 256.],\n",
              "         [256., 256., 256., 256.],\n",
              "         [256., 256., 256., 256.]]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.var(unbiased=True,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[16., 17., 18., 19.],\n",
              "         [20., 21., 22., 23.],\n",
              "         [24., 25., 26., 27.],\n",
              "         [28., 29., 30., 31.]],\n",
              "\n",
              "        [[64., 65., 66., 67.],\n",
              "         [68., 69., 70., 71.],\n",
              "         [72., 73., 74., 75.],\n",
              "         [76., 77., 78., 79.]]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0.9498, 1.0702, 1.0165]), tensor([0.9498, 1.0702, 1.0165]))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "class MyBatchNorm2d(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1,\n",
        "                 affine=True, track_running_stats=True):\n",
        "        super(MyBatchNorm2d, self).__init__(\n",
        "            num_features, eps, momentum, affine, track_running_stats)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self._check_input_dim(input)\n",
        "\n",
        "        exponential_average_factor = 0.0\n",
        "\n",
        "        if self.training and self.track_running_stats:\n",
        "            if self.num_batches_tracked is not None:\n",
        "                self.num_batches_tracked += 1\n",
        "                if self.momentum is None:  # use cumulative moving average\n",
        "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
        "                else:  # use exponential moving average\n",
        "                    exponential_average_factor = self.momentum\n",
        "\n",
        "        # calculate running estimates\n",
        "        if self.training:\n",
        "            mean = input.mean(dim=[0, 2, 3])\n",
        "            # use biased var in train\n",
        "            var = input.var([0, 2, 3], unbiased=False)\n",
        "            n = input.numel() / input.size(1)\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = exponential_average_factor * mean + (1 - exponential_average_factor) * self.running_mean\n",
        "                # update running_var with unbiased var\n",
        "                self.running_var = exponential_average_factor * var * n / (n - 1) + (1 - exponential_average_factor) * self.running_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        input = (input - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\n",
        "        if self.affine:\n",
        "            input = input * self.weight[None, :, None, None] + self.bias[None, :, None, None]\n",
        "\n",
        "        return input\n",
        "    \n",
        "norm = nn.BatchNorm2d(3,momentum=0.1)\n",
        "norm1 = MyBatchNorm2d(3,momentum=0.1)\n",
        "d = torch.randn(2, 3, 2, 3)\n",
        "\n",
        "test = norm(d)\n",
        "test = norm1(d)\n",
        "norm1.running_var, norm.running_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.9498, 1.0702, 1.0165])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gamma = 0.1\n",
        "mean = d.mean([0, 2, 3])\n",
        "var = d.var([0, 2, 3], unbiased=False)\n",
        "n = d.numel() / d.size(1)\n",
        "running_variance = gamma * var * n / (n - 1) + ( 1 - gamma) * torch.ones_like(var)\n",
        "running_variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.9046, 1.1333, 1.0313])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = norm(d)\n",
        "\n",
        "norm.running_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.9498, 1.0702, 1.0165])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm1.running_var"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All parameters are equal!\n",
            "All parameters are equal!\n",
            "tensor([3.3909, 3.4120, 3.3850]) tensor([3.3909, 3.4120, 3.3850])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([3.4507, 3.4710, 3.4473]) tensor([3.4507, 3.4710, 3.4473])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([6.6752, 6.7449, 6.6850]) tensor([6.6752, 6.7449, 6.6850])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([9.6064, 9.6532, 9.6202]) tensor([9.6064, 9.6532, 9.6202])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([10.2415, 10.2822, 10.2644]) tensor([10.2415, 10.2822, 10.2644])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([15.6107, 15.6859, 15.6040]) tensor([15.6107, 15.6859, 15.6040])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([20.4277, 20.5349, 20.4101]) tensor([20.4277, 20.5349, 20.4101])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([18.7880, 18.8805, 18.7695]) tensor([18.7880, 18.8805, 18.7695])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([17.8137, 17.8932, 17.7964]) tensor([17.8137, 17.8932, 17.7964])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
            "tensor([17.6380, 17.7047, 17.6042]) tensor([17.6380, 17.7047, 17.6042])\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(3.5763e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(3.5763e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n",
            "All parameters are equal!\n",
            "Max diff:  tensor(9.5367e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Comparison of manual BatchNorm2d layer implementation in Python and\n",
        "nn.BatchNorm2d\n",
        "\n",
        "@author: ptrblck\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def compare_bn(bn1, bn2):\n",
        "    err = False\n",
        "    if not torch.allclose(bn1.running_mean, bn2.running_mean):\n",
        "        print('Diff in running_mean: {} vs {}'.format(\n",
        "            bn1.running_mean, bn2.running_mean))\n",
        "        err = True\n",
        "\n",
        "    if not torch.allclose(bn1.running_var, bn2.running_var):\n",
        "        print('Diff in running_var: {} vs {}'.format(\n",
        "            bn1.running_var, bn2.running_var))\n",
        "        err = True\n",
        "\n",
        "    if bn1.affine and bn2.affine:\n",
        "        if not torch.allclose(bn1.weight, bn2.weight):\n",
        "            print('Diff in weight: {} vs {}'.format(\n",
        "                bn1.weight, bn2.weight))\n",
        "            err = True\n",
        "\n",
        "        if not torch.allclose(bn1.bias, bn2.bias):\n",
        "            print('Diff in bias: {} vs {}'.format(\n",
        "                bn1.bias, bn2.bias))\n",
        "            err = True\n",
        "\n",
        "    if not err:\n",
        "        print('All parameters are equal!')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Init BatchNorm layers\n",
        "my_bn = MyBatchNorm2d(3, affine=True)\n",
        "bn = nn.BatchNorm2d(3, affine=True)\n",
        "\n",
        "compare_bn(my_bn, bn)  # weight and bias should be different\n",
        "# Load weight and bias\n",
        "my_bn.load_state_dict(bn.state_dict())\n",
        "compare_bn(my_bn, bn)\n",
        "\n",
        "# Run train\n",
        "for _ in range(10):\n",
        "    scale = torch.randint(1, 10, (1,)).float()\n",
        "    bias = torch.randint(-10, 10, (1,)).float()\n",
        "    x = torch.randn(10, 3, 100, 100) * scale + bias\n",
        "    out1 = my_bn(x)\n",
        "    out2 = bn(x)\n",
        "    print(my_bn.running_var,bn.running_var)\n",
        "    compare_bn(my_bn, bn)\n",
        "\n",
        "    torch.allclose(out1, out2)\n",
        "    print('Max diff: ', (out1 - out2).abs().max())\n",
        "\n",
        "# Run eval\n",
        "my_bn.eval()\n",
        "bn.eval()\n",
        "for _ in range(10):\n",
        "    scale = torch.randint(1, 10, (1,)).float()\n",
        "    bias = torch.randint(-10, 10, (1,)).float()\n",
        "    x = torch.randn(10, 3, 100, 100) * scale + bias\n",
        "    out1 = my_bn(x)\n",
        "    out2 = bn(x)\n",
        "    compare_bn(my_bn, bn)\n",
        "\n",
        "    torch.allclose(out1, out2)\n",
        "    print('Max diff: ', (out1 - out2).abs().max())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
