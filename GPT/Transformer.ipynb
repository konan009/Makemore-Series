{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt','r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the string: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the string:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print( data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([111540]), torch.Size([1003854]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "val_data.shape, train_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "        \n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits,loss = model(xb,yb)\n",
    "\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "         [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "         [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "         [25, 17, 27, 10,  0, 21,  1, 54]]),\n",
       " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
       "         [53, 56,  1, 58, 46, 39, 58,  1],\n",
       "         [58,  1, 58, 46, 39, 58,  1, 46],\n",
       "         [17, 27, 10,  0, 21,  1, 54, 39]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:5], yb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss : 4.7040\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "    xb,yb = get_batch('train')\n",
    "    logits , loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 1000 == 0 :\n",
    "        print(f\" Loss : { loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "olylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4116, val loss 4.4022\n",
      "step 100: train loss 2.6568, val loss 2.6670\n",
      "step 199: train loss 2.5086, val loss 2.5057\n",
      "\n",
      "And thak bridcowi, w OLon, bth\n",
      "\n",
      "Hiset bobe d e.\n",
      "S:\n",
      "O:\n",
      "ISM:\n",
      "CELTauss:\n",
      "Wanthar u qorthe.t? cedXlas ate awice my.\n",
      "\n",
      "HDEd com orou w owhavetof is h bot mil ndilin,\n",
      "\n",
      "W ireees, hcin lat Hotid ov te, ane t nd.\n",
      "Girileranses l lind me l.\n",
      "Hhule ce wiry:\n",
      "ARO: aiss hew ye wllinde noridpetelavd wh m\n",
      "\n",
      "P:\n",
      "The teankleo Windo wh tCeiibal w ati dourisET ge, ieed t sousower; te\n",
      "\n",
      "Ank d nterupt f son; irist m:\n",
      "ENurey aleronth, af Pre? ge nom.\n",
      "AHKINLIE!\n",
      "K,\n",
      "Sb&is:\n",
      "Aadsal ace E: fond din cst amaraney Iry ts chan y ce.\n",
      "Okene ton, bemary.\n",
      "You I ou w yom soun anghy torenomes t dene mrdande; st auline win lletisod, wiourco f spyy stouthas l.\n",
      "Tu Eias d thapein, thorued Pee heemovet ad dpansc toros cok ovet s.\n",
      "\n",
      "O:\n",
      "Hume he be feRUCatos:\n",
      "Whit CINoveces nisthen ld, Ie n, moxlone.\n",
      "\n",
      "An! beaket aghercoENO: wam k s withesel nem, de s wllo noncarid, dcesedI surd?\n",
      "TI ide\n",
      "MHpo venond, d Ce?\n",
      "Fy fe thisoou tiusorthe nof t e sutan wiporth ou wute t, ditth $ alepe Blisenke, ou s h O, thean, de wat d&eive wout ir fofu;\n",
      "\n",
      "Ankceg oueee rtane itu fo. fasit fet Henk; ge when the is,, h pr thitanofalnon bay n or, andemen, meseveminds s; te woriingin ie- toulig m We a wow ndechcal fore goE:\n",
      "Sy senInirstha catint de d ofind yan be ketimisl menat mlinor, Reide biw mngise.\n",
      "\n",
      "\n",
      "CARe wo IICis;tha d tho y'ollHh weertmourul lalop!g the olirconou jeea d f toupthMmu th?\n",
      "\n",
      "Shaing mas, h n fo h c; dou.\n",
      "I tir dond'dlll dyouy meras be:\n",
      "Hung o nvonbust RI ot noml veabae; I n angale itandnu t nm Thieerte y, d.\n",
      "I, ge, issoyieyou,\n",
      "BoRO:\n",
      ": ulat we\n",
      "Thins; s ookey souk bt, derars, es s;\n",
      "RI th d oakaveat cee Cis t.\n",
      "Thaee d whed sed we arist arevove wacy prei? s derkLor kng&en thin.\n",
      "\n",
      "\n",
      "':\n",
      "FWHAncome Iwuld. ar tithear I, gist athe couf ahir, bmoaas o'd.\n",
      "\n",
      "Thithese haw He hernd en:\n",
      "Bur wiflouth se ma aI h corpous cilt, omemonge,\n",
      "Aoul, tald wu t rere w yonfaiowalo' I o!\n",
      "Aldu w y houliowevelist g s:\n",
      "Silf gpr outo wheaau w wk:\n",
      "Anow.\n",
      "Whur woury pm my, mand awonseat,\n",
      "J a. dtuVance fldo, J sePR pat nel:\n",
      "Ingis sh wh'ed.\n",
      "Whathee s k:\n",
      "Tour h henide a \n",
      "Mare\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 100\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "# # print the number of parameters in the model\n",
    "# print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[43,  1, 57, 58, 39, 58, 43,  0, 27, 44,  1, 58, 46, 39, 58,  1, 47, 52,\n",
       "          58, 43, 45, 56, 47, 58, 63,  1, 61, 46, 47, 41, 46,  1],\n",
       "         [ 1, 47, 57,  1, 52, 53, 61,  1, 52, 53, 58,  1, 44, 39, 47, 56,  8,  0,\n",
       "          26, 53, 61,  1, 30, 53, 51, 43, 53,  1, 47, 57,  1, 40],\n",
       "         [ 1, 40, 59, 57, 47, 52, 43, 57, 57,  8,  0,  0, 13, 33, 18, 21, 16, 21,\n",
       "          33, 31, 10,  0, 27, 52, 50, 63,  1, 58, 46, 43, 47, 56],\n",
       "         [52,  1, 46, 43, 56,  1, 58, 43, 52, 42, 43, 56,  1, 46, 43, 39, 56, 58,\n",
       "           1, 58, 46, 43,  1, 39, 57, 54, 47, 56, 47, 52, 45,  1],\n",
       "         [53, 61,  1, 58, 46, 63,  1, 40, 53, 42, 63,  1, 47, 52,  1, 39, 52, 53,\n",
       "          58, 46, 43, 56,  1, 56, 53, 53, 51,  0, 13, 52, 42,  1]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 1, 57, 58, 39, 58, 43,  0, 27, 44,  1, 58, 46, 39, 58,  1, 47, 52, 58,\n",
       "          43, 45, 56, 47, 58, 63,  1, 61, 46, 47, 41, 46,  1, 57],\n",
       "         [47, 57,  1, 52, 53, 61,  1, 52, 53, 58,  1, 44, 39, 47, 56,  8,  0, 26,\n",
       "          53, 61,  1, 30, 53, 51, 43, 53,  1, 47, 57,  1, 40, 43],\n",
       "         [40, 59, 57, 47, 52, 43, 57, 57,  8,  0,  0, 13, 33, 18, 21, 16, 21, 33,\n",
       "          31, 10,  0, 27, 52, 50, 63,  1, 58, 46, 43, 47, 56,  1],\n",
       "         [ 1, 46, 43, 56,  1, 58, 43, 52, 42, 43, 56,  1, 46, 43, 39, 56, 58,  1,\n",
       "          58, 46, 43,  1, 39, 57, 54, 47, 56, 47, 52, 45,  1, 44],\n",
       "         [61,  1, 58, 46, 63,  1, 40, 53, 42, 63,  1, 47, 52,  1, 39, 52, 53, 58,\n",
       "          46, 43, 56,  1, 56, 53, 53, 51,  0, 13, 52, 42,  1, 58]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:5],yb[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embedding : \n",
      "torch.Size([16, 32, 50])\n",
      " \n",
      " Block \n",
      "torch.Size([16, 32, 50])\n",
      "tensor([[[-0.0877,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.2076, -0.0370,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0327, -0.1299,  0.1717,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.0585, -0.0083,  0.1286,  ...,  0.0676,    -inf,    -inf],\n",
      "         [-0.0042, -0.1099, -0.0409,  ...,  0.0309,  0.0357,    -inf],\n",
      "         [-0.1831, -0.1089,  0.1763,  ..., -0.0987,  0.0213,  0.1541]],\n",
      "\n",
      "        [[ 0.1414,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1958,  0.0262,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0461, -0.2466,  0.1717,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1804, -0.3215, -0.0194,  ...,  0.0444,    -inf,    -inf],\n",
      "         [-0.0464, -0.2878,  0.2892,  ..., -0.0179,  0.1024,    -inf],\n",
      "         [ 0.0951,  0.4579,  0.0857,  ..., -0.2054, -0.0467, -0.1128]],\n",
      "\n",
      "        [[ 0.1414,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.2427,  0.0291,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0342, -0.1960,  0.0251,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1861, -0.0077,  0.0830,  ..., -0.0805,    -inf,    -inf],\n",
      "         [-0.0283, -0.1309,  0.0341,  ..., -0.0595,  0.2088,    -inf],\n",
      "         [-0.0612, -0.1133, -0.0042,  ..., -0.2693, -0.1487,  0.0393]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1657,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1581, -0.1037,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1177,  0.0972,  0.1151,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1350,  0.1887,  0.1787,  ...,  0.1647,    -inf,    -inf],\n",
      "         [-0.0287, -0.2065, -0.0734,  ..., -0.1714,  0.0543,    -inf],\n",
      "         [ 0.1953, -0.2223, -0.0946,  ..., -0.3199, -0.0935,  0.1971]],\n",
      "\n",
      "        [[-0.0241,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0467,  0.0262,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1035, -0.2466,  0.1717,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.0443, -0.0863, -0.0871,  ...,  0.0553,    -inf,    -inf],\n",
      "         [ 0.1297, -0.4561,  0.2195,  ..., -0.2469,  0.0900,    -inf],\n",
      "         [-0.0072,  0.3169,  0.1616,  ..., -0.0705, -0.0516,  0.1146]],\n",
      "\n",
      "        [[-0.1855,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0789, -0.1361,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1557,  0.0837,  0.1116,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0153, -0.1394, -0.1956,  ...,  0.0737,    -inf,    -inf],\n",
      "         [ 0.0551, -0.1390,  0.2170,  ..., -0.2906,  0.0076,    -inf],\n",
      "         [ 0.0155, -0.1107,  0.0169,  ..., -0.1587, -0.0995,  0.0866]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "tensor([[[-0.0158,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0063,  0.1503,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0929, -0.0322,  0.0263,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0105, -0.0867,  0.0368,  ..., -0.0694,    -inf,    -inf],\n",
      "         [-0.2512, -0.4998,  0.0523,  ...,  0.3013,  0.0008,    -inf],\n",
      "         [-0.0036,  0.1875, -0.0454,  ..., -0.0210,  0.0305,  0.1844]],\n",
      "\n",
      "        [[-0.0552,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.2282, -0.1164,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0964,  0.3035,  0.0263,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1808, -0.1019,  0.0701,  ..., -0.0670,    -inf,    -inf],\n",
      "         [-0.1899, -0.2081, -0.1869,  ..., -0.0782, -0.1863,    -inf],\n",
      "         [-0.1513,  0.1456,  0.0934,  ...,  0.0672, -0.0980,  0.0023]],\n",
      "\n",
      "        [[-0.0552,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0950, -0.0972,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0211,  0.0269,  0.0546,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2811,  0.0315,  0.0618,  ..., -0.0179,    -inf,    -inf],\n",
      "         [-0.4089, -0.1144, -0.3177,  ..., -0.1740, -0.0700,    -inf],\n",
      "         [ 0.1855, -0.0467,  0.0840,  ...,  0.0748,  0.0988,  0.1582]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0046,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0446,  0.1706,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0693,  0.0923, -0.1267,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0361, -0.0728, -0.0908,  ...,  0.1062,    -inf,    -inf],\n",
      "         [-0.0559, -0.1173,  0.0310,  ...,  0.0464, -0.0864,    -inf],\n",
      "         [-0.0739,  0.0843,  0.2075,  ..., -0.0125,  0.0048,  0.0552]],\n",
      "\n",
      "        [[-0.1871,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1935, -0.1164,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0021,  0.3035,  0.0263,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0511, -0.2123,  0.1708,  ...,  0.0115,    -inf,    -inf],\n",
      "         [ 0.0340, -0.1066, -0.0779,  ...,  0.0553, -0.0145,    -inf],\n",
      "         [-0.3154,  0.2577,  0.0746,  ..., -0.0587,  0.1254, -0.1658]],\n",
      "\n",
      "        [[ 0.0351,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1580, -0.0234,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1286, -0.0366,  0.1367,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.1564, -0.0219, -0.0324,  ..., -0.0065,    -inf,    -inf],\n",
      "         [-0.0499, -0.1760, -0.2347,  ...,  0.0715, -0.1091,    -inf],\n",
      "         [-0.0900,  0.3133,  0.2131,  ..., -0.0460,  0.2021, -0.0438]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "tensor([[[ 0.1161,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0503,  0.0551,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1739,  0.0563, -0.0392,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0993,  0.0108,  0.2335,  ..., -0.1124,    -inf,    -inf],\n",
      "         [-0.3430,  0.1555, -0.3520,  ..., -0.1037, -0.0809,    -inf],\n",
      "         [ 0.0502,  0.2228, -0.3127,  ...,  0.2807, -0.3068, -0.3774]],\n",
      "\n",
      "        [[-0.0458,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0756,  0.0582,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0283, -0.0005, -0.0392,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0466, -0.0937,  0.0712,  ..., -0.0679,    -inf,    -inf],\n",
      "         [-0.1389,  0.1853, -0.3420,  ..., -0.1549, -0.4776,    -inf],\n",
      "         [ 0.0058,  0.1823, -0.1816,  ..., -0.0352, -0.2916, -0.1702]],\n",
      "\n",
      "        [[-0.0458,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0330, -0.1288,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0787, -0.1315, -0.0744,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0383, -0.0447, -0.0428,  ...,  0.1302,    -inf,    -inf],\n",
      "         [-0.0633, -0.0254, -0.1395,  ..., -0.1698,  0.0108,    -inf],\n",
      "         [ 0.0076,  0.2137, -0.3471,  ..., -0.0856, -0.4404, -0.0644]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0773,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0304,  0.0506,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1516,  0.0072,  0.1007,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2373,  0.0192,  0.0851,  ..., -0.1717,    -inf,    -inf],\n",
      "         [ 0.0770, -0.2335, -0.4472,  ..., -0.1216, -0.1793,    -inf],\n",
      "         [-0.0999,  0.0815,  0.0569,  ...,  0.2143, -0.0565,  0.0341]],\n",
      "\n",
      "        [[-0.1652,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0173,  0.0582,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.2422, -0.0005, -0.0392,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.0630, -0.1164,  0.2259,  ...,  0.0842,    -inf,    -inf],\n",
      "         [ 0.0155,  0.0265, -0.1227,  ..., -0.4013, -0.1381,    -inf],\n",
      "         [-0.1731,  0.2420, -0.3344,  ...,  0.1180,  0.1186, -0.0190]],\n",
      "\n",
      "        [[-0.1582,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1380, -0.1510,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0720, -0.0628,  0.0038,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0153, -0.0944, -0.0526,  ...,  0.0824,    -inf,    -inf],\n",
      "         [ 0.0755, -0.0350, -0.1552,  ...,  0.1968, -0.2920,    -inf],\n",
      "         [ 0.1020,  0.1736, -0.1251,  ...,  0.2618, -0.1235, -0.1537]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "tensor([[[ 0.2682,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1987,  0.1621,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0830, -0.1888, -0.0599,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1511,  0.1562,  0.0814,  ...,  0.2327,    -inf,    -inf],\n",
      "         [ 0.0104,  0.0625,  0.1114,  ...,  0.1619,  0.1795,    -inf],\n",
      "         [-0.2311, -0.3100, -0.0522,  ..., -0.2291, -0.1961, -0.2043]],\n",
      "\n",
      "        [[-0.2125,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0550,  0.0715,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0090, -0.2006, -0.0599,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2791, -0.2434,  0.1818,  ..., -0.0338,    -inf,    -inf],\n",
      "         [-0.3016,  0.0656,  0.0503,  ...,  0.1624, -0.2389,    -inf],\n",
      "         [-0.2904, -0.1460,  0.0335,  ..., -0.0148,  0.0045,  0.1483]],\n",
      "\n",
      "        [[-0.2125,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0593,  0.0430,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1197, -0.1008, -0.1754,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1219, -0.2474,  0.0316,  ...,  0.0989,    -inf,    -inf],\n",
      "         [-0.1110,  0.2367,  0.0088,  ..., -0.0429,  0.0437,    -inf],\n",
      "         [-0.0561, -0.1575,  0.0770,  ...,  0.1363, -0.0594,  0.2109]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0654,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1094,  0.0353,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1868, -0.2485, -0.0783,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2060, -0.1959,  0.1993,  ..., -0.1910,    -inf,    -inf],\n",
      "         [-0.0450,  0.1861,  0.1401,  ...,  0.1483, -0.2358,    -inf],\n",
      "         [-0.1465, -0.1691,  0.0663,  ...,  0.0473, -0.1351,  0.1519]],\n",
      "\n",
      "        [[-0.0074,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0218,  0.0715,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0508, -0.2006, -0.0599,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2558, -0.2833,  0.1262,  ..., -0.2515,    -inf,    -inf],\n",
      "         [-0.1666,  0.2026, -0.0948,  ..., -0.0010, -0.1358,    -inf],\n",
      "         [-0.1795, -0.1558,  0.1154,  ...,  0.1424,  0.0264,  0.1709]],\n",
      "\n",
      "        [[-0.1215,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0215,  0.0720,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0823, -0.1425,  0.0023,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2879, -0.0843,  0.0208,  ..., -0.1488,    -inf,    -inf],\n",
      "         [ 0.0374, -0.0448,  0.1487,  ..., -0.1019,  0.0942,    -inf],\n",
      "         [-0.1320, -0.0408,  0.0891,  ..., -0.0813,  0.1787,  0.2323]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "tensor([[[ 0.0062,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0062, -0.0326,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1669,  0.1281,  0.3270,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1162, -0.1234,  0.1679,  ..., -0.0233,    -inf,    -inf],\n",
      "         [ 0.0805, -0.0308,  0.1546,  ...,  0.0788,  0.0445,    -inf],\n",
      "         [ 0.1739,  0.0755, -0.0661,  ..., -0.1256,  0.1674,  0.0128]],\n",
      "\n",
      "        [[ 0.1046,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0393,  0.1811,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0433,  0.0185,  0.3270,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0722, -0.0551,  0.1509,  ...,  0.0428,    -inf,    -inf],\n",
      "         [ 0.0177,  0.2002, -0.1950,  ..., -0.0895,  0.1062,    -inf],\n",
      "         [-0.0498, -0.0445,  0.0985,  ...,  0.1459,  0.0229,  0.0521]],\n",
      "\n",
      "        [[ 0.1046,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0641, -0.0744,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0625, -0.0269,  0.0386,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0607,  0.1065,  0.1503,  ..., -0.0788,    -inf,    -inf],\n",
      "         [-0.0041,  0.1076, -0.1872,  ...,  0.0580,  0.2511,    -inf],\n",
      "         [ 0.0069,  0.0411,  0.0347,  ...,  0.1699,  0.1675,  0.2369]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0709,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1775, -0.0945,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0554,  0.0740,  0.2603,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1001, -0.2867, -0.1278,  ..., -0.0526,    -inf,    -inf],\n",
      "         [ 0.0842, -0.1524, -0.2612,  ..., -0.1187,  0.0132,    -inf],\n",
      "         [ 0.4281, -0.1772, -0.0158,  ...,  0.0055, -0.0236,  0.6519]],\n",
      "\n",
      "        [[-0.1605,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0442,  0.1811,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1999,  0.0185,  0.3270,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.4544,  0.0547, -0.1471,  ..., -0.0072,    -inf,    -inf],\n",
      "         [ 0.0781, -0.1671,  0.0240,  ..., -0.0295,  0.0531,    -inf],\n",
      "         [ 0.1807,  0.0395,  0.1358,  ...,  0.1972,  0.0395,  0.3254]],\n",
      "\n",
      "        [[ 0.0984,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.2100, -0.0286,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0640,  0.1667,  0.0642,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0682,  0.1160,  0.0877,  ..., -0.1294,    -inf,    -inf],\n",
      "         [ 0.2086,  0.1095,  0.0161,  ...,  0.0181,  0.1805,    -inf],\n",
      "         [ 0.1217,  0.0411, -0.0730,  ...,  0.0437,  0.1038, -0.0148]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "torch.Size([16, 32, 50])\n",
      " Block \n",
      "torch.Size([16, 32, 50])\n",
      "tensor([[[-0.1516,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0361,  0.0889,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0238, -0.0776,  0.1224,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.1035, -0.1034, -0.0378,  ..., -0.1236,    -inf,    -inf],\n",
      "         [ 0.3390,  0.1160,  0.0175,  ..., -0.0143, -0.1176,    -inf],\n",
      "         [-0.0301,  0.1230, -0.1941,  ...,  0.0196, -0.0060, -0.0320]],\n",
      "\n",
      "        [[-0.0510,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0686,  0.0061,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0458, -0.1046,  0.1113,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.1531, -0.0701,  0.0224,  ...,  0.1765,    -inf,    -inf],\n",
      "         [ 0.1119, -0.0837, -0.0596,  ..., -0.0590,  0.0892,    -inf],\n",
      "         [-0.0640,  0.0600,  0.0143,  ..., -0.0249, -0.0794, -0.0399]],\n",
      "\n",
      "        [[-0.0510,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0095, -0.0408,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1748,  0.1502,  0.0198,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.0593, -0.0856,  0.1082,  ..., -0.0243,    -inf,    -inf],\n",
      "         [ 0.1018, -0.0416,  0.0674,  ..., -0.1350, -0.1786,    -inf],\n",
      "         [-0.1386, -0.1015, -0.0604,  ...,  0.2435,  0.0405,  0.0859]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1185,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.3666, -0.0276,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1372, -0.1401,  0.0290,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0896,  0.0595, -0.1674,  ...,  0.0300,    -inf,    -inf],\n",
      "         [ 0.2133, -0.0384, -0.0401,  ...,  0.0256,  0.0530,    -inf],\n",
      "         [-0.1440,  0.0880,  0.1254,  ...,  0.2391, -0.0422, -0.1352]],\n",
      "\n",
      "        [[-0.0352,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0649, -0.0150,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0417, -0.1011,  0.1360,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0170, -0.0123, -0.1516,  ...,  0.1678,    -inf,    -inf],\n",
      "         [ 0.0226, -0.1354, -0.0250,  ..., -0.0746, -0.0945,    -inf],\n",
      "         [ 0.0062,  0.0320, -0.0507,  ...,  0.1956, -0.0772, -0.0077]],\n",
      "\n",
      "        [[-0.0083,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1450, -0.1881,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1557, -0.0234,  0.0016,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0310, -0.1112, -0.0149,  ...,  0.1376,    -inf,    -inf],\n",
      "         [ 0.1857, -0.0981,  0.1266,  ..., -0.2486, -0.0673,    -inf],\n",
      "         [ 0.0016, -0.0211,  0.0036,  ..., -0.0541,  0.0163,  0.0441]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "tensor([[[ 0.0736,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0325,  0.0909,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0924, -0.1231,  0.0509,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1368, -0.0143, -0.0208,  ...,  0.0755,    -inf,    -inf],\n",
      "         [-0.0626,  0.0792, -0.0646,  ..., -0.0560, -0.2536,    -inf],\n",
      "         [-0.2864,  0.2559,  0.0527,  ..., -0.0543,  0.0853,  0.1766]],\n",
      "\n",
      "        [[ 0.2135,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.2210, -0.1233,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0769,  0.0616,  0.0810,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.1796, -0.0745,  0.1477,  ...,  0.1139,    -inf,    -inf],\n",
      "         [-0.0969, -0.0150, -0.0394,  ..., -0.0070,  0.4074,    -inf],\n",
      "         [ 0.2170,  0.2432,  0.0680,  ...,  0.2116,  0.0370,  0.2030]],\n",
      "\n",
      "        [[ 0.2135,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0667,  0.0494,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1315, -0.1878,  0.0697,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0450,  0.0273,  0.1331,  ..., -0.0976,    -inf,    -inf],\n",
      "         [-0.2881,  0.0131, -0.1158,  ...,  0.0564,  0.0425,    -inf],\n",
      "         [ 0.0304,  0.0475,  0.1087,  ..., -0.0321,  0.0008,  0.2833]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1344,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1627, -0.2219,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0679, -0.0623, -0.1224,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1064,  0.1773,  0.0979,  ...,  0.0906,    -inf,    -inf],\n",
      "         [-0.0810, -0.0110, -0.0473,  ...,  0.0867,  0.0642,    -inf],\n",
      "         [ 0.0508,  0.3933,  0.0900,  ...,  0.1061, -0.0218,  0.1575]],\n",
      "\n",
      "        [[ 0.2550,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1297, -0.1034,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0909,  0.0671,  0.0747,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1717, -0.0590, -0.0086,  ...,  0.1325,    -inf,    -inf],\n",
      "         [-0.2956, -0.1366, -0.0239,  ...,  0.1727, -0.1619,    -inf],\n",
      "         [-0.1263,  0.1004,  0.0914,  ...,  0.1915,  0.0284,  0.1075]],\n",
      "\n",
      "        [[ 0.0336,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0036, -0.0367,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1390, -0.1042, -0.0066,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2353, -0.0871, -0.0044,  ...,  0.0618,    -inf,    -inf],\n",
      "         [-0.0333,  0.0178,  0.0256,  ..., -0.0933, -0.0569,    -inf],\n",
      "         [-0.0244, -0.0963, -0.0519,  ...,  0.1148, -0.1964,  0.0271]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "tensor([[[-0.2218,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0789, -0.1598,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1027, -0.2653,  0.3678,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0403,  0.0992, -0.2446,  ...,  0.1148,    -inf,    -inf],\n",
      "         [ 0.2380,  0.2026, -0.1798,  ..., -0.1771, -0.1638,    -inf],\n",
      "         [-0.1501, -0.1506,  0.1252,  ...,  0.1326,  0.1255, -0.0923]],\n",
      "\n",
      "        [[-0.1465,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0122, -0.0243,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0315, -0.1691,  0.3485,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2338, -0.0218,  0.1189,  ..., -0.1535,    -inf,    -inf],\n",
      "         [-0.0746,  0.0527, -0.0633,  ...,  0.0127, -0.1551,    -inf],\n",
      "         [-0.2501, -0.0342, -0.1123,  ...,  0.0571, -0.0276,  0.0973]],\n",
      "\n",
      "        [[-0.1465,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1727,  0.0556,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1210, -0.0306,  0.1750,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2765,  0.0170, -0.0433,  ..., -0.0777,    -inf,    -inf],\n",
      "         [ 0.0670,  0.0562, -0.2276,  ...,  0.0453, -0.0814,    -inf],\n",
      "         [ 0.2049,  0.0540,  0.3416,  ..., -0.2283,  0.0097,  0.3372]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0036,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0547,  0.0628,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0982, -0.1274,  0.2450,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.2205,  0.0086, -0.1210,  ...,  0.1498,    -inf,    -inf],\n",
      "         [ 0.0910,  0.0226,  0.0323,  ..., -0.0032, -0.0885,    -inf],\n",
      "         [-0.1556,  0.0371,  0.0131,  ...,  0.1405,  0.0629,  0.2207]],\n",
      "\n",
      "        [[ 0.0985,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0178, -0.0033,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1064, -0.1469,  0.3824,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.0816,  0.1331, -0.2817,  ...,  0.0766,    -inf,    -inf],\n",
      "         [-0.0197,  0.1269, -0.3897,  ...,  0.4015,  0.0674,    -inf],\n",
      "         [-0.1035, -0.0779,  0.0607,  ...,  0.1223,  0.0605,  0.0189]],\n",
      "\n",
      "        [[ 0.0448,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.3615, -0.1287,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0516, -0.1396,  0.1747,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2682, -0.0631,  0.4580,  ...,  0.0030,    -inf,    -inf],\n",
      "         [-0.0059, -0.1086, -0.1336,  ..., -0.0601, -0.1462,    -inf],\n",
      "         [ 0.1536, -0.1625,  0.0547,  ...,  0.0172, -0.0942,  0.0826]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "tensor([[[ 6.0981e-02,        -inf,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [-1.1718e-02,  6.9243e-02,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [ 1.6835e-02, -1.1728e-01, -1.1204e-01,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         ...,\n",
      "         [-6.9204e-02, -7.0404e-02, -8.3131e-02,  ...,  2.3664e-03,\n",
      "                 -inf,        -inf],\n",
      "         [ 2.4460e-04, -1.5596e-02, -1.1239e-01,  ..., -5.7135e-02,\n",
      "           2.6653e-02,        -inf],\n",
      "         [-1.3992e-02,  1.1678e-01, -1.7590e-02,  ...,  1.5709e-01,\n",
      "          -3.9919e-02, -1.1379e-01]],\n",
      "\n",
      "        [[ 1.9330e-01,        -inf,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [-4.1746e-02,  1.2186e-02,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [-1.1434e-01,  1.0504e-03, -9.8963e-02,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         ...,\n",
      "         [-1.2375e-01,  8.2207e-03, -2.2669e-01,  ..., -2.7197e-03,\n",
      "                 -inf,        -inf],\n",
      "         [ 5.7398e-02,  1.2617e-01,  4.7953e-02,  ...,  1.2336e-01,\n",
      "           1.7146e-01,        -inf],\n",
      "         [-8.2030e-03, -2.5448e-02,  1.1393e-01,  ..., -3.9874e-02,\n",
      "           4.5762e-02, -9.1992e-03]],\n",
      "\n",
      "        [[ 1.9330e-01,        -inf,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [-1.1135e-01, -5.5320e-03,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [ 1.8151e-02, -1.5054e-01,  1.7250e-01,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         ...,\n",
      "         [-5.7503e-02, -8.6793e-02, -8.7534e-02,  ...,  7.3412e-02,\n",
      "                 -inf,        -inf],\n",
      "         [-1.0803e-02,  1.1088e-02,  8.5072e-02,  ...,  9.0091e-02,\n",
      "           1.7828e-01,        -inf],\n",
      "         [ 6.9873e-02, -2.7164e-01, -1.1242e-01,  ..., -1.6286e-01,\n",
      "          -1.0422e-01,  1.2011e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.2658e-02,        -inf,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [-7.2518e-02, -1.5156e-01,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [-1.3713e-01, -3.3441e-02,  1.5258e-02,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         ...,\n",
      "         [-1.3293e-03, -1.1600e-01,  1.2873e-01,  ...,  1.1096e-01,\n",
      "                 -inf,        -inf],\n",
      "         [-1.4014e-01,  1.1954e-01,  2.0217e-01,  ..., -5.7823e-03,\n",
      "           1.0697e-01,        -inf],\n",
      "         [-1.2959e-01,  1.8055e-01,  1.2256e-01,  ..., -5.5372e-02,\n",
      "          -1.0897e-01, -2.0691e-01]],\n",
      "\n",
      "        [[ 2.2801e-02,        -inf,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [-7.5541e-02,  2.9646e-02,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [ 8.7099e-02, -2.7492e-03, -8.9740e-02,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         ...,\n",
      "         [ 4.1731e-02, -3.6191e-02, -1.2587e-01,  ...,  6.0772e-02,\n",
      "                 -inf,        -inf],\n",
      "         [-2.5031e-02,  1.1047e-01, -5.9989e-02,  ...,  2.4477e-01,\n",
      "           1.1239e-01,        -inf],\n",
      "         [ 6.9375e-02,  2.6764e-02, -5.7192e-02,  ..., -6.5611e-02,\n",
      "          -1.0313e-01, -1.4802e-01]],\n",
      "\n",
      "        [[ 1.9314e-01,        -inf,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [-1.2795e-02, -2.9916e-02,        -inf,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         [ 7.3098e-02,  7.6057e-02,  2.1589e-01,  ...,        -inf,\n",
      "                 -inf,        -inf],\n",
      "         ...,\n",
      "         [-1.5357e-01, -1.8337e-01, -1.8032e-01,  ...,  1.3571e-01,\n",
      "                 -inf,        -inf],\n",
      "         [-1.2562e-01,  1.2560e-01, -4.4006e-02,  ..., -6.0138e-02,\n",
      "           2.8699e-01,        -inf],\n",
      "         [ 3.3330e-02,  2.5791e-02,  1.0548e-01,  ...,  4.4876e-02,\n",
      "          -7.9314e-02, -1.1876e-04]]], device='cuda:0',\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "tensor([[[ 0.1023,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.0016,  0.0197,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.1158, -0.2907, -0.1104,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [-0.0549, -0.1487, -0.0491,  ..., -0.1045,    -inf,    -inf],\n",
      "         [ 0.0640,  0.0797,  0.0301,  ..., -0.0655,  0.0431,    -inf],\n",
      "         [ 0.0601,  0.1463, -0.0700,  ...,  0.0655,  0.0740,  0.2515]],\n",
      "\n",
      "        [[-0.0284,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0353,  0.0287,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.3628, -0.0200, -0.0848,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0712, -0.0881,  0.1291,  ..., -0.3980,    -inf,    -inf],\n",
      "         [-0.0145, -0.0940, -0.1828,  ...,  0.0800,  0.1164,    -inf],\n",
      "         [-0.0046,  0.0114, -0.1475,  ...,  0.0736,  0.0865,  0.2213]],\n",
      "\n",
      "        [[-0.0284,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0213, -0.0020,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.5006, -0.3062, -0.1573,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.2303,  0.0753,  0.0555,  ..., -0.2674,    -inf,    -inf],\n",
      "         [ 0.0156,  0.0013, -0.1498,  ...,  0.0668,  0.0807,    -inf],\n",
      "         [-0.0143, -0.1074, -0.1108,  ..., -0.0295, -0.0548,  0.0234]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0206,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1835, -0.0516,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.2018, -0.0124, -0.2905,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0047, -0.0513, -0.0390,  ...,  0.0770,    -inf,    -inf],\n",
      "         [-0.2181, -0.0523, -0.2336,  ..., -0.0051,  0.0759,    -inf],\n",
      "         [-0.1833, -0.1164,  0.0113,  ...,  0.0534, -0.0463, -0.0197]],\n",
      "\n",
      "        [[ 0.1068,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.0925,  0.0284,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [-0.2017, -0.0155, -0.0997,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.0544,  0.0147, -0.0213,  ..., -0.0586,    -inf,    -inf],\n",
      "         [-0.0616, -0.2351, -0.0570,  ..., -0.1570,  0.0644,    -inf],\n",
      "         [-0.0823,  0.0391,  0.0131,  ...,  0.0413, -0.0736,  0.0221]],\n",
      "\n",
      "        [[ 0.0963,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1503, -0.0358,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [ 0.1351, -0.0785,  0.1532,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [ 0.1594,  0.0856, -0.2785,  ..., -0.3678,    -inf,    -inf],\n",
      "         [ 0.0089, -0.0637, -0.0893,  ..., -0.0838, -0.0450,    -inf],\n",
      "         [ 0.0352,  0.0261,  0.0175,  ...,  0.0398, -0.0187, -0.0779]]],\n",
      "       device='cuda:0', grad_fn=<MaskedFillBackward0>)\n",
      "torch.Size([16, 32, 10])\n",
      "torch.Size([16, 32, 50])\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, out_dim, bias=False)\n",
    "        self.query = nn.Linear(n_embd, out_dim, bias=False)\n",
    "        self.value = nn.Linear(n_embd, out_dim, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        print(wei)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        print(out.shape)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\" Block \")\n",
    "        x = x + self.self_attention(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "# super simple bigram model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # Learnable Positional Embedding \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        print(\"Token Embedding : \")\n",
    "        print(tok_emb.shape)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        print(\" \")\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # x = self.ln_f(x) # (B,T,C)\n",
    "        # logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        # if targets is None:\n",
    "        #     loss = None\n",
    "        # else:\n",
    "        #     B, T, C = logits.shape\n",
    "        #     logits = logits.view(B*T, C)\n",
    "        #     targets = targets.view(B*T)\n",
    "        #     loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # return logits, loss\n",
    "n_embd = 50\n",
    "n_head = 5\n",
    "n_layer = 2\n",
    "# hyperparameters\n",
    "batch_size = 10 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 200\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "model(xb,yb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
